# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xYbIQ7y4SI-lSrvKCdCMGBuyw542qeei
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.datasets import fetch_openml

mnist = fetch_openml('mnist_784', version=1)

X = mnist['data']
y = mnist['target'].astype(np.int64)

#one hot encoding
def one_hot(y, num_classes=10):
  m = y.shape[0]
  encode = np.zeros((m, num_classes))
  encode[np.arange(m), y] = 1
  return encode
Y = one_hot(y)

#defining neural net
np.random.seed(42)

input_size = 784
hidden_size = 128
output_size = 10

W1 = np.random.randn(input_size, hidden_size) * np.sqrt(1. / input_size)
b1 = np.zeros((1, hidden_size))

W2 = np.random.randn(hidden_size, output_size) * np.sqrt(1. / hidden_size)
b2 = np.zeros((1, output_size))

#activation functions
def relu(x):
  return np.maximum(0, x)

def softmax(x):
  exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
  return exp_x / np.sum(exp_x, axis=1, keepdims=True)

#forward pass
def forward_pass(X, W1, b1, W2, b2):
  #hidden layer
  Z1 = np.dot(X, W1) + b1
  A1 = relu(Z1)

  #output layer
  Z2 = np.dot(Z1, W2) + b2
  A2 = softmax(Z2)
  return Z1, A1, Z2, A2

Z1, A1, Z2, A2 = forward_pass(X, W1, b1, W2, b2)

def compute_loss(Y, A2):
  m = Y.shape[0]
  log_probs = -np.log(A2 + 1e-8)
  loss = np.sum(Y * log_probs) / m
  return loss

loss = compute_loss(Y, A2)
print(loss)

def relu_derivative(Z):
  return Z > 0

#backward pass
def backward_pass(X, Y, Z1, A1, Z2, A2, W2):
  m = X.shape[0]
  dZ2 = (A2 - Y)
  dW2 = np.dot(A1.T, dZ2) / m
  db2 = np.sum(dZ2, axis=0, keepdims=True) / m

  #hidden layer
  dA1 = np.dot(dZ2, W2.T)
  dZ1 = dA1 * relu_derivative(Z1)
  dW1 = np.dot(X.T, dZ1) / m
  db1 = np.sum(dZ1, axis=0, keepdims=True) / m

  return dW1, db1, dW2, db2

alpha = 0.1
epochs = 1000
for epoch in range(epochs):
  #forward pass
  Z1, A1, Z2, A2 = forward_pass(X, W1, b1, W2, b2)

  loss = compute_loss(Y, A2)

  #backward pass
  dW1, db1, dW2, db2 = backward_pass(X, Y, Z1, A1, Z2, A2, W2)

  W1 -= alpha * dW1
  b1 -= alpha * db1
  W2 -= alpha * dW2
  b2 -= alpha * db2

  print(f"Epoch {epoch}: Loss = {loss:.4f}")